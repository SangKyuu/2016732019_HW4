{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2016732019_임상규_assignment_14.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Assignment\n",
        "a) 아래 예제 코드를 이용해 텍스트 입력의 숫자 변환 과정을 체크한다\n",
        "\n",
        "b) testset의 임의 입력을 LongTensor로 변환해 학습 완료된 모델에 입력해보고, 결과가 어떠한지 체크한다 (하단의 출력 결과를 참조)\n",
        "\n"
      ],
      "metadata": {
        "id": "A22RwFdlyiLV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M3Poyc7yXSZ",
        "outputId": "8cbbdfad-8a09-4bde-e1aa-68a5a501b3ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.legacy import data, datasets \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "# parameters\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "training_epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMDB Dataset\n",
        "\n",
        "# data.Field 설명 #\n",
        "# sequential인자 : TEXT는 Sequential 데이터라 True, Lable은 비Sequential이라 False로 설정\n",
        "# batch_first : Batch를 우선시 하여, Tensor 크기를 (BATCH_SIZE, 문장의 최대 길이)로 설정\n",
        "# lower : 소문자 전환 인자\n",
        "\n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
        "LABEL = data.Field(sequential=False, batch_first=True)\n",
        "trainset, testset = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "# data.Field.build_vocab() 라이브러리\n",
        "# 문장 내 단어와 Integer index 를 매칭시키는 단어장(vocab)을 생성 == 워드 임베딩을 위한 Vocab 생성\n",
        "# <UNK> = 0, <PAD> = 1 토큰도 추가.\n",
        "# min_freq : 최소 5번 이상 등장한 단어들만 사전에 담겠다는 것. \n",
        "# 5번 미만으로 등장하는 단어는 UNK라는 토큰으로 대체\n",
        "\n",
        "TEXT.build_vocab(trainset, min_freq=5)# TEXT 데이터를 기반으로 Vocab 생성\n",
        "LABEL.build_vocab(trainset)# LABEL 데이터를 기반으로 Vocab 생성\n",
        "\n",
        "# 학습용 데이터를 학습셋 80% 검증셋 20% 로 나누기\n",
        "trainset, valset = trainset.split(split_ratio=0.8)\n",
        "# 매 배치마다 비슷한 길이에 맞춰 줄 수 있도록 iterator 정의\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "        (trainset, valset, testset), batch_size=batch_size,\n",
        "        shuffle=True, repeat=False)\n",
        "\n",
        "vocab_size = len(TEXT.vocab)\n",
        "n_classes = 2 # Positive, Negative Class가 두 개\n",
        "\n",
        "print(\"[TrainSet]: %d [ValSet]: %d [TestSet]: %d [Vocab]: %d [Classes] %d\"\n",
        "      % (len(trainset),len(valset), len(testset), vocab_size, n_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKkE-Cndyzz8",
        "outputId": "30c0cad6-d29f-4638-fd65-e08247423e8a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84.1M/84.1M [00:01<00:00, 47.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrainSet]: 20000 [ValSet]: 5000 [TestSet]: 25000 [Vocab]: 46159 [Classes] 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicGRU(nn.Module):\n",
        "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
        "        super(BasicGRU, self).__init__()\n",
        "        self.n_layers = n_layers # 일반적으로는 2\n",
        "\n",
        "        #n_vocab : Vocab 안에 있는 단어의 개수, embed_dim : 임베딩 된 단어 텐서가 갖는 차원 값(dimension)\n",
        "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
        "\n",
        "        # hidden vector의 dimension과 dropout 정의\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        #앞에서 정의한 하이퍼 파라미터를 넣어 GRU 정의\n",
        "        self.gru = nn.GRU(embed_dim, self.hidden_dim,\n",
        "                          num_layers=self.n_layers,\n",
        "                          batch_first=True)\n",
        "        \n",
        "        #Input: GRU의 hidden vector(context), Output : Class probability vector\n",
        "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input data: 한 batch 내 모든 영화 평가 데이터\n",
        "        \n",
        "        x = self.embed(x)# 영화 평 임베딩\n",
        "        x, _ = self.gru(x)  # [i, b, h] 출력값 :  (batch_size, 입력 x의 길이, hidden_dim)\n",
        "\n",
        "        # h_t : Batch 내 모든 sequential hidden state vector의 제일 마지막 토큰을 내포한 (batch_size, 1, hidden_dim)형태의 텐서 추출\n",
        "        # 다른 의미로 영화 리뷰 배열들을 압축한 hidden state vector\n",
        "        h_t = x[:,-1,:]\n",
        "\n",
        "        self.dropout(h_t)# dropout 설정 후, \n",
        "\n",
        "        # linear layer의 입력으로 주고, 각 클래스 별 결과 logit을 생성.\n",
        "        out = self.out(h_t)  # [b, h] -> [b, o]\n",
        "        return out"
      ],
      "metadata": {
        "id": "0m1f4DHvy0_1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# contruct model\n",
        "model = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(device)\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# train\n",
        "for epoch in range(training_epochs):\n",
        "  avg_cost = 0\n",
        "  for batch in train_iter:\n",
        "    X, Y = batch.text.to(device), batch.label.to(device)\n",
        "    Y.data.sub_(1)\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(X)\n",
        "    cost = criterion(hypothesis, Y)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    avg_cost += cost / batch_size\n",
        "  print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\n",
        "print('Learning Finished!')\n",
        "\n",
        "# model save\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/model_s1.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9KiDWe9y4xI",
        "outputId": "7614463e-6b48-4d35-adc7-01569bcd8e3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch:    1] cost = 3.41286826\n",
            "[Epoch:    2] cost = 3.39303303\n",
            "[Epoch:    3] cost = 3.29124904\n",
            "[Epoch:    4] cost = 2.28365111\n",
            "[Epoch:    5] cost = 1.29460239\n",
            "Learning Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model load\n",
        "model_new = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(device)\n",
        "model_new.load_state_dict(torch.load('/content/drive/MyDrive/model_s1.pt'))\n",
        "\n",
        "corrects = 0\n",
        "for batch in val_iter:\n",
        "  x,y = batch.text.to(device), batch.label.to(device)\n",
        "  y.data.sub_(1)\n",
        "  hypothesis = model_new(x)\n",
        "  corrects += (hypothesis.max(1)[1].view(y.size()).data == y.data).sum() \n",
        "\n",
        "print('accuracy = ', corrects/len(val_iter.dataset)*100.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aprJaVFy7qj",
        "outputId": "d29ee241-ee42-459a-9a12-af43115275cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy =  tensor(86.2200, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text, input_label = testset[3].text, testset[3].label\n",
        "print(input_text)\n",
        "print(TEXT.vocab[input_text[0]])\n",
        "print(TEXT.vocab[input_text[1]])\n",
        "print(TEXT.vocab[input_text[2]])\n",
        "print(TEXT.vocab[input_text[3]])\n",
        "print(TEXT.vocab[input_text[4]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0NNyKC-00De",
        "outputId": "aae2a437-2a89-4e73-8222-7675df7effc9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'word', 'impossible', 'has', 'led', 'many', 'to', 'select', 'a', 'particular', 'view', 'concerning', 'any', 'incredible', 'task.', 'in', '1927,', 'it', 'was', 'believed', 'no', 'man', 'could', 'fly', 'the', 'breath', 'of', 'the', 'atlantic', 'ocean.', 'many', 'had', 'tried', 'but', 'failed', 'and', 'some', 'even', 'gave', 'their', 'lives', 'to', 'the', 'effort.', 'nevertheless,', 'it', 'had', 'to', 'be', 'done', 'as', 'every', 'challenge', 'needs', 'to', 'be', 'met', 'with', 'equal', 'determination.', 'such', 'then', 'is', 'the', 'heart', 'of', 'this', 'movie', 'called', '\"the', 'spirit', 'of', 'st.', 'louis.\"', 'the', 'actor', 'chosen', 'for', 'this', 'historic', 'film', 'is', 'none', 'other', 'than', \"america's\", 'own', 'james', 'stewart', 'who', 'convincingly', 'plays', 'charles', 'lindbergh.', 'although', 'there', 'are', 'many', 'facets', 'of', \"lindbergh's\", 'life,', 'the', 'segment', 'featured', 'here', 'is', 'his', 'efforts', 'to', 'be', 'the', 'first', 'man', 'to', 'fly', 'across', 'the', 'atlantic.', 'the', 'story', 'is', 'an', 'interesting', 'one', 'and', 'for', \"stewards'\", 'fans', 'compelling', 'to', 'say', 'the', 'least.', 'seeking', 'enough', 'funds', 'to', 'build', 'a', 'special', 'aircraft,', 'to', 'the', 'fateful', 'decision', 'to', 'began', 'the', 'journey', 'on', 'a', 'gloomy', 'day', 'in', 'may', '1927,', \"'luck\", \"lindy'\", 'as', 'he', 'was', 'christened,', 'endured', 'enormous', 'risks,', 'which', 'are', 'featured', 'in', 'this', 'superb', 'film.', 'other', 'notables', 'which', 'helped', 'make', 'this', 'film', 'believable', 'are', 'murray', 'hamilton', 'who', 'plays', 'bud', 'gurney,', 'bartlett', 'robinson', 'as', 'ben', 'mahoney,', 'arthur', 'space', 'and', 'charles', 'watts', 'as', 'o.w.', 'schultz.', 'the', 'sum', 'total', 'of', 'this', 'now', 'famous', 'movie', 'is', 'that', 'despite', 'poor', 'endorsement', 'on', 'its', 'debut,', 'it', 'has', 'since', 'become', 'a', 'classic', 'in', \"it's\", 'own', 'right.', 'well', 'done!', '****']\n",
            "2\n",
            "822\n",
            "1241\n",
            "41\n",
            "1563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load test text\n",
        "model_new = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(device)\n",
        "model_new.load_state_dict(torch.load('/content/drive/MyDrive/model_s1.pt'))\n",
        "\n",
        "input_embdedding = [TEXT.vocab[i] for i in input_text]"
      ],
      "metadata": {
        "id": "Py5tdX-Z3_q7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_embdedding)\n",
        "print(input_label)\n",
        "input_embdedding = torch.tensor(input_embdedding, device=device, dtype=torch.long)\n",
        "print(input_embdedding)\n",
        "\n",
        "input_embdedding = input_embdedding.unsqueeze(0)\n",
        "pred = model_new(input_embdedding)\n",
        "print(pred.max(1)[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE7IAzTJ9yce",
        "outputId": "21c6ad62-446a-4893-e178-731e9499ae18"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 822, 1241, 41, 1563, 100, 6, 15279, 3, 928, 798, 3842, 92, 1172, 13361, 8, 0, 12, 14, 3082, 60, 160, 90, 2698, 2, 3450, 5, 2, 13867, 31349, 100, 62, 738, 18, 1272, 4, 45, 55, 441, 59, 517, 6, 2, 3966, 3853, 12, 62, 6, 28, 247, 15, 158, 3285, 677, 6, 28, 2036, 17, 3594, 42121, 130, 96, 7, 2, 657, 5, 10, 20, 437, 222, 1348, 5, 5472, 0, 2, 342, 2398, 16, 10, 6315, 24, 7, 603, 80, 68, 5377, 209, 569, 1729, 36, 5814, 270, 1489, 0, 281, 52, 22, 100, 17618, 5, 0, 747, 2, 2615, 2516, 198, 7, 21, 2864, 6, 28, 2, 82, 160, 6, 2698, 563, 2, 0, 2, 75, 7, 32, 246, 30, 4, 16, 0, 516, 1660, 6, 142, 2, 5127, 3160, 221, 13181, 6, 1713, 3, 301, 0, 6, 2, 12791, 2543, 6, 1769, 2, 1540, 19, 3, 8767, 352, 8, 186, 0, 0, 0, 15, 27, 14, 0, 13970, 4354, 0, 63, 22, 2516, 8, 10, 1099, 139, 80, 0, 63, 1609, 91, 10, 24, 1155, 22, 7178, 6401, 36, 270, 8153, 0, 0, 6894, 15, 1075, 43689, 1930, 1060, 4, 1489, 25320, 15, 0, 0, 2, 3184, 917, 5, 10, 204, 772, 20, 7, 11, 446, 350, 0, 19, 83, 9096, 12, 41, 227, 359, 3, 409, 8, 44, 209, 1867, 107, 19245, 2342]\n",
            "pos\n",
            "tensor([    2,   822,  1241,    41,  1563,   100,     6, 15279,     3,   928,\n",
            "          798,  3842,    92,  1172, 13361,     8,     0,    12,    14,  3082,\n",
            "           60,   160,    90,  2698,     2,  3450,     5,     2, 13867, 31349,\n",
            "          100,    62,   738,    18,  1272,     4,    45,    55,   441,    59,\n",
            "          517,     6,     2,  3966,  3853,    12,    62,     6,    28,   247,\n",
            "           15,   158,  3285,   677,     6,    28,  2036,    17,  3594, 42121,\n",
            "          130,    96,     7,     2,   657,     5,    10,    20,   437,   222,\n",
            "         1348,     5,  5472,     0,     2,   342,  2398,    16,    10,  6315,\n",
            "           24,     7,   603,    80,    68,  5377,   209,   569,  1729,    36,\n",
            "         5814,   270,  1489,     0,   281,    52,    22,   100, 17618,     5,\n",
            "            0,   747,     2,  2615,  2516,   198,     7,    21,  2864,     6,\n",
            "           28,     2,    82,   160,     6,  2698,   563,     2,     0,     2,\n",
            "           75,     7,    32,   246,    30,     4,    16,     0,   516,  1660,\n",
            "            6,   142,     2,  5127,  3160,   221, 13181,     6,  1713,     3,\n",
            "          301,     0,     6,     2, 12791,  2543,     6,  1769,     2,  1540,\n",
            "           19,     3,  8767,   352,     8,   186,     0,     0,     0,    15,\n",
            "           27,    14,     0, 13970,  4354,     0,    63,    22,  2516,     8,\n",
            "           10,  1099,   139,    80,     0,    63,  1609,    91,    10,    24,\n",
            "         1155,    22,  7178,  6401,    36,   270,  8153,     0,     0,  6894,\n",
            "           15,  1075, 43689,  1930,  1060,     4,  1489, 25320,    15,     0,\n",
            "            0,     2,  3184,   917,     5,    10,   204,   772,    20,     7,\n",
            "           11,   446,   350,     0,    19,    83,  9096,    12,    41,   227,\n",
            "          359,     3,   409,     8,    44,   209,  1867,   107, 19245,  2342],\n",
            "       device='cuda:0')\n",
            "tensor([1], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yeGJ9y5l57Lt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}