{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2016732019_임상규_assignment_12.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXYmADDksBYK"
      },
      "source": [
        "### 3 Assignment\n",
        "###다음 3개의 문장을 batch data로 활용해 RNN을 학습해보자 (아래의 미완성 코드, 위 실습코드, 실행결과를 참고)\n",
        "\n",
        "- 'howareyou'\n",
        "- 'whats up?'\n",
        "- 'iamgreat.'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDLrTy8rsKBd",
        "outputId": "e2f3ffd2-37aa-42ac-f7b3-839c977df578"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(100)\n",
        "\n",
        "# Dictionary\n",
        "sample_sentences = ['howareyou', 'whats up?', 'iamgreat.']\n",
        "char_set = list(set(''.join(sample_sentences)))\n",
        "dic = {c: i for i, c in enumerate(char_set)}\n",
        "\n",
        "# Parameters\n",
        "dic_size = len(dic)\n",
        "input_size = dic_size\n",
        "hidden_size = dic_size\n",
        "\n",
        "# Dataset setting\n",
        "input_batch = []\n",
        "target_batch = []\n",
        "\n",
        "for sentence in sample_sentences:\n",
        "    x_data = [dic[c] for c in sentence[:-1]]\n",
        "    x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
        "    y_data = [dic[c] for c in sentence[1:]]\n",
        "\n",
        "    input_batch.append(x_one_hot)\n",
        "    target_batch.append(y_data)\n",
        "\n",
        "# To torch tensors\n",
        "X = torch.FloatTensor(input_batch)\n",
        "Y = torch.LongTensor(target_batch)\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "# Model\n",
        "learning_rate = 0.005\n",
        "training_epochs = 500\n",
        "model = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = nn.CrossEntropyLoss()    # Softmax\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# train\n",
        "for epoch in range(training_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs, _status = model(X)\n",
        "    loss = criterion(outputs.reshape(-1, dic_size), Y.reshape(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 20 == 1:\n",
        "        result = outputs.data.numpy().argmax(axis=2)\n",
        "        for i, s in enumerate(result):\n",
        "          result_str = ''.join([char_set[c] for c in s])\n",
        "          print(f'epoch {epoch}, loss {loss.item()}, {i+1}st prediction: {result_str}, true Y: {sample_sentences[i][1:]}')\n",
        "\n",
        "        print()\n",
        "\n",
        "result = outputs.data.numpy().argmax(axis=2)\n",
        "for sentence in result:\n",
        "  print(''.join([char_set[c] for c in np.squeeze(sentence)]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 8, 17])\n",
            "torch.Size([3, 8])\n",
            "epoch 1, loss 2.8435986042022705, 1st prediction: otsttttt, true Y: owareyou\n",
            "epoch 1, loss 2.8435986042022705, 2st prediction: yotetstm, true Y: hats up?\n",
            "epoch 1, loss 2.8435986042022705, 3st prediction: toosttmt, true Y: amgreat.\n",
            "\n",
            "epoch 21, loss 2.3926846981048584, 1st prediction: oteeeaau, true Y: owareyou\n",
            "epoch 21, loss 2.3926846981048584, 2st prediction: toteua.m, true Y: hats up?\n",
            "epoch 21, loss 2.3926846981048584, 3st prediction: ttreeaaa, true Y: amgreat.\n",
            "\n",
            "epoch 41, loss 2.0774576663970947, 1st prediction: otareyat, true Y: owareyou\n",
            "epoch 41, loss 2.0774576663970947, 2st prediction: hate at?, true Y: hats up?\n",
            "epoch 41, loss 2.0774576663970947, 3st prediction: tmrreaa., true Y: amgreat.\n",
            "\n",
            "epoch 61, loss 1.860697627067566, 1st prediction: owareyau, true Y: owareyou\n",
            "epoch 61, loss 1.860697627067566, 2st prediction: hats ap?, true Y: hats up?\n",
            "epoch 61, loss 1.860697627067566, 3st prediction: hmrreaa., true Y: amgreat.\n",
            "\n",
            "epoch 81, loss 1.707586646080017, 1st prediction: owareyau, true Y: owareyou\n",
            "epoch 81, loss 1.707586646080017, 2st prediction: hats ap?, true Y: hats up?\n",
            "epoch 81, loss 1.707586646080017, 3st prediction: hmgreat., true Y: amgreat.\n",
            "\n",
            "epoch 101, loss 1.6014288663864136, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 101, loss 1.6014288663864136, 2st prediction: hats ap?, true Y: hats up?\n",
            "epoch 101, loss 1.6014288663864136, 3st prediction: hmgreat., true Y: amgreat.\n",
            "\n",
            "epoch 121, loss 1.5254355669021606, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 121, loss 1.5254355669021606, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 121, loss 1.5254355669021606, 3st prediction: hmgreat., true Y: amgreat.\n",
            "\n",
            "epoch 141, loss 1.4702329635620117, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 141, loss 1.4702329635620117, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 141, loss 1.4702329635620117, 3st prediction: hmgreat., true Y: amgreat.\n",
            "\n",
            "epoch 161, loss 1.429481029510498, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 161, loss 1.429481029510498, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 161, loss 1.429481029510498, 3st prediction: amgreat., true Y: amgreat.\n",
            "\n",
            "epoch 181, loss 1.3985122442245483, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 181, loss 1.3985122442245483, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 181, loss 1.3985122442245483, 3st prediction: amgreat., true Y: amgreat.\n",
            "\n",
            "epoch 201, loss 1.3743581771850586, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 201, loss 1.3743581771850586, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 201, loss 1.3743581771850586, 3st prediction: amgreat., true Y: amgreat.\n",
            "\n",
            "epoch 221, loss 1.3549596071243286, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 221, loss 1.3549596071243286, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 221, loss 1.3549596071243286, 3st prediction: amgreat., true Y: amgreat.\n",
            "\n",
            "epoch 241, loss 1.3389261960983276, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 241, loss 1.3389261960983276, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 241, loss 1.3389261960983276, 3st prediction: amgreat., true Y: amgreat.\n",
            "\n",
            "epoch 261, loss 1.325347900390625, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 261, loss 1.325347900390625, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 261, loss 1.325347900390625, 3st prediction: amgreat., true Y: amgreat.\n",
            "\n",
            "epoch 281, loss 1.3136314153671265, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 281, loss 1.3136314153671265, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 281, loss 1.3136314153671265, 3st prediction: amgreat., true Y: amgreat.\n",
            "\n",
            "epoch 301, loss 1.3033840656280518, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 301, loss 1.3033840656280518, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 301, loss 1.3033840656280518, 3st prediction: amgreat., true Y: amgreat.\n",
            "\n",
            "epoch 321, loss 1.2943395376205444, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 321, loss 1.2943395376205444, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 321, loss 1.2943395376205444, 3st prediction: amgreat., true Y: amgreat.\n",
            "\n",
            "epoch 341, loss 1.2863094806671143, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 341, loss 1.2863094806671143, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 341, loss 1.2863094806671143, 3st prediction: amgreat., true Y: amgreat.\n",
            "\n",
            "epoch 361, loss 1.2791519165039062, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 361, loss 1.2791519165039062, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 361, loss 1.2791519165039062, 3st prediction: amgreat., true Y: amgreat.\n",
            "\n",
            "epoch 381, loss 1.2727470397949219, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 381, loss 1.2727470397949219, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 381, loss 1.2727470397949219, 3st prediction: amgreat., true Y: amgreat.\n",
            "\n",
            "epoch 401, loss 1.2669848203659058, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 401, loss 1.2669848203659058, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 401, loss 1.2669848203659058, 3st prediction: amgreat., true Y: amgreat.\n",
            "\n",
            "epoch 421, loss 1.2617669105529785, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 421, loss 1.2617669105529785, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 421, loss 1.2617669105529785, 3st prediction: amgreat., true Y: amgreat.\n",
            "\n",
            "epoch 441, loss 1.2570081949234009, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 441, loss 1.2570081949234009, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 441, loss 1.2570081949234009, 3st prediction: amgreat., true Y: amgreat.\n",
            "\n",
            "epoch 461, loss 1.2526365518569946, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 461, loss 1.2526365518569946, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 461, loss 1.2526365518569946, 3st prediction: amgreat., true Y: amgreat.\n",
            "\n",
            "epoch 481, loss 1.2485918998718262, 1st prediction: owareyou, true Y: owareyou\n",
            "epoch 481, loss 1.2485918998718262, 2st prediction: hats up?, true Y: hats up?\n",
            "epoch 481, loss 1.2485918998718262, 3st prediction: amgreat., true Y: amgreat.\n",
            "\n",
            "owareyou\n",
            "hats up?\n",
            "amgreat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo5y5MTuuAl6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}